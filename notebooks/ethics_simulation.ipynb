{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ethics Simulation\n",
        "\n",
        "This notebook provides a lightweight workflow to:\n",
        "1) Generate a synthetic dataset with sensitive attributes\n",
        "2) Compute basic fairness metrics across groups\n",
        "3) Run a naive bias score on text\n",
        "4) Apply a simple mitigation (reweighting) and re-evaluate\n",
        "5) Save an audit log for reproducibility\n",
        "\n",
        "_Note: This is a scaffolding notebook meant for demonstration and extension._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Imports & config\n",
        "import os, json, math, random, uuid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "OUTPUT_DIR = os.path.join('outputs')\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "RUN_ID = str(uuid.uuid4())\n",
        "RUN_TS = datetime.utcnow().isoformat() + 'Z'\n",
        "\n",
        "print('RUN_ID:', RUN_ID)\n",
        "print('Timestamp (UTC):', RUN_TS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Create / Load Synthetic Dataset\n",
        "We simulate a small classification dataset with a sensitive attribute `group` and a text field to test a simple bias score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def synthesize(n=400):\n",
        "    groups = np.random.choice(['A','B'], size=n, p=[0.5, 0.5])\n",
        "    # True label with slight base imbalance\n", 
        "    y = (np.random.rand(n) > 0.45).astype(int)\n",
        "    # Introduce group-related outcome skew (for demo)\n",
        "    y = np.where((groups=='B') & (np.random.rand(n) < 0.1), 0, y)\n",
        "\n",
        "    phrases_pos = [\n",
        "        'excellent service','great product','helpful and responsive',\n",
        "        'smooth experience','impressive results','works as expected'\n",
        "    ]\n",
        "    phrases_neg = [\n",
        "        'poor quality','confusing UI','unhelpful support',\n",
        "        'buggy and slow','not worth it','bad experience'\n",
        "    ]\n",
        "    sensitive_terms = ['pregnant','age 62','member of X group','religion mentioned']\n",
        "\n",
        "    texts = []\n",
        "    for i in range(n):\n",
        "        base = random.choice(phrases_pos if y[i]==1 else phrases_neg)\n",
        "        if random.random() < 0.15:\n",
        "            base += ' | ' + random.choice(sensitive_terms)\n",
        "        texts.append(base)\n",
        "    return pd.DataFrame({'text': texts, 'label': y, 'group': groups})\n",
        "\n",
        "df = synthesize(400)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Basic Fairness Metrics\n",
        "We compute:\n",
        "- **Prevalence** per group\n",
        "- **Positive Rate** per group\n",
        "- **Disparate Impact Ratio (DIR)** = min(positive_rate_group)/max(positive_rate_group)\n",
        "- **Absolute Rate Difference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def group_rates(frame, y_col='label', g_col='group'):\n",
        "    stats = frame.groupby(g_col)[y_col].agg(['mean','count'])\n",
        "    stats = stats.rename(columns={'mean':'positive_rate','count':'n'})\n",
        "    # DIR and absolute difference\n",
        "    rates = stats['positive_rate'].values\n",
        "    if len(rates) >= 2:\n",
        "        dir_ratio = float(np.min(rates) / max(np.max(rates), 1e-9))\n",
        "        abs_diff = float(np.max(rates) - np.min(rates))\n",
        "    else:\n",
        "        dir_ratio, abs_diff = float('nan'), float('nan')\n",
        "    return stats.reset_index(), dir_ratio, abs_diff\n",
        "\n",
        "stats_before, dir_before, diff_before = group_rates(df)\n",
        "print('Group stats BEFORE:')\n",
        "display(stats_before)\n",
        "print('DIR (before):', round(dir_before, 3))\n",
        "print('Abs rate diff (before):', round(diff_before, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Text Bias Score (Naive)\n",
        "A quick heuristic that flags the presence of sensitive terms in text. **This is for demo only**—replace with a stronger policy or model-based detector in production."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "FLAG_TERMS = ['pregnant','age 62','member of X group','religion']\n",
        "\n",
        "def bias_score(text):\n",
        "    t = text.lower()\n",
        "    return sum(1 for w in FLAG_TERMS if w in t)\n",
        "\n",
        "df['bias_score'] = df['text'].apply(bias_score)\n",
        "df['has_sensitive_flag'] = (df['bias_score'] > 0).astype(int)\n",
        "df[['text','bias_score','has_sensitive_flag','group','label']].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Group-wise Bias Flag Rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "flag_stats = df.groupby('group')['has_sensitive_flag'].mean().reset_index(name='flag_rate')\n",
        "display(flag_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Simple Mitigation: Reweighting\n",
        "We simulate a mitigation by reweighting groups to equalize positive rates.\n",
        "This is an illustrative technique—use proper debiasing methods for real workloads (e.g., rebalancing datasets, threshold adjustments, fairness-aware loss)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def compute_weights(frame, g_col='group', y_col='label'):\n",
        "    grp = frame.groupby(g_col)[y_col].mean()\n",
        "    # Target positive rate = global positive rate\n",
        "    target = frame[y_col].mean()\n",
        "    w = {}\n",
        "    for g, rate in grp.items():\n",
        "        # Avoid div-by-zero; clamp weight between 0.5 and 2.0 for stability\n",
        "        val = target / max(rate, 1e-6)\n",
        "        w[g] = float(np.clip(val, 0.5, 2.0))\n",
        "    return w\n",
        "\n",
        "weights = compute_weights(df)\n",
        "weights"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apply weights and recompute weighted metrics. For simplicity, we compute a **weighted positive rate** by group and evaluate new DIR / absolute diff."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def weighted_rates(frame, weights, y_col='label', g_col='group'):\n",
        "    tmp = frame.copy()\n",
        "    tmp['w'] = tmp[g_col].map(weights)\n",
        "    # Weighted positive rate per group\n",
        "    grp = tmp.groupby(g_col).apply(lambda x: (x[y_col]*x['w']).sum() / max(x['w'].sum(), 1e-9)).reset_index(name='w_positive_rate')\n",
        "    # Weighted DIR and abs diff\n",
        "    rates = grp['w_positive_rate'].values\n",
        "    if len(rates) >= 2:\n",
        "        dir_ratio = float(np.min(rates) / max(np.max(rates), 1e-9))\n",
        "        abs_diff = float(np.max(rates) - np.min(rates))\n",
        "    else:\n",
        "        dir_ratio, abs_diff = float('nan'), float('nan')\n",
        "    return grp, dir_ratio, abs_diff\n",
        "\n",
        "w_stats, dir_after, diff_after = weighted_rates(df, weights)\n",
        "print('Weighted group stats AFTER:')\n",
        "display(w_stats)\n",
        "print('DIR (after):', round(dir_after, 3))\n",
        "print('Abs rate diff (after):', round(diff_after, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Results Summary & Audit Log\n",
        "We capture inputs, metrics, and parameters for traceability. This supports the transparency and reproducibility goals in the ethics framework."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "summary = {\n",
        "    'run_id': RUN_ID,\n",
        "    'timestamp_utc': RUN_TS,\n",
        "    'n_rows': int(len(df)),\n",
        "    'metrics_before': {\n",
        "        'dir': float(dir_before),\n",
        "        'abs_rate_diff': float(diff_before)\n",
        "    },\n",
        "    'metrics_after': {\n",
        "        'dir': float(dir_after),\n",
        "        'abs_rate_diff': float(diff_after)\n",
        "    },\n",
        "    'group_stats_before': stats_before.to_dict(orient='records'),\n",
        "    'group_stats_after_weighted': w_stats.to_dict(orient='records'),\n",
        "    'weights': weights,\n",
        "    'flag_terms': list(set([t.lower() for t in ['pregnant','age 62','member of X group','religion']])),\n",
        "    'notes': 'Synthetic demo; use domain-appropriate fairness tooling for production.'\n",
        "}\n",
        "\n",
        "audit_path = os.path.join(OUTPUT_DIR, f'ethics_simulation_audit_{RUN_ID}.json')\n",
        "with open(audit_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print('Audit log saved →', audit_path)\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Next Steps\n",
        "- Replace naive bias detectors with robust policies or moderation/filters.\n",
        "- Add group-aware evaluation for precision/recall/FPR/FNR.\n",
        "- Explore dataset rebalancing, counterfactual augmentation, or threshold calibration.\n",
        "- Integrate with a model card in `/docs` describing risks, mitigations, and evaluation."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}