{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentiment Model Training (Complete)\n",
        "\n",
        "A compact, reproducible pipeline for binary sentiment classification using **TF-IDF + Logistic Regression**.\n",
        "\n",
        "**What this does:**\n",
        "1) Load dataset from `datasets/sentiment_training.json` (or use a tiny fallback).\n",
        "2) Stratified train/validation/test split.\n",
        "3) Small hyperparameter search over C and n-grams.\n",
        "4) Evaluate with accuracy, F1, ROC-AUC, confusion matrix.\n",
        "5) Save model + summary JSON to `outputs/`.\n",
        "6) Provide `infer(texts)` helper and quick error analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Imports & setup\n",
        "import os, json, uuid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "import joblib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "RUN_ID = str(uuid.uuid4())\n",
        "OUT_DIR = Path('outputs'); OUT_DIR.mkdir(exist_ok=True)\n",
        "DATA_PATH = Path('datasets/sentiment_training.json')\n",
        "print('RUN_ID:', RUN_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load data\n",
        "Expected JSON schema (as in repo):\n",
        "```json\n",
        "{ \"text\": [\"great\", ...], \"label\": [1, 0, ...] }\n",
        "```\n",
        "The expanded version also includes metadata fields; we handle both."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "if DATA_PATH.exists():\n",
        "    with open(DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "        payload = json.load(f)\n",
        "    # handle either flat schema or metadata-wrapped\n",
        "    if isinstance(payload, dict) and 'text' in payload and 'label' in payload:\n",
        "        texts, labels = payload['text'], payload['label']\n",
        "    else:\n",
        "        raise ValueError('Unsupported JSON format for sentiment_training.json')\n",
        "else:\n",
        "    # Fallback tiny dataset\n",
        "    texts = ['great product','bad experience','okay','terrible quality','excellent support','meh']\n",
        "    labels = [1,0,1,0,1,0]\n",
        "\n",
        "df = pd.DataFrame({'text': texts, 'label': labels})\n",
        "print(df.head(), '\\nRows:', len(df))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Train/Val/Test split (stratified)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "X = df['text'].astype(str).values\n",
        "y = np.asarray(df['label']).astype(int)\n",
        "\n",
        "# 80% train, 10% val, 10% test\n",
        "X_train, X_tmp, y_train, y_tmp = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_tmp, y_tmp, test_size=0.5, random_state=42, stratify=y_tmp\n",
        ")\n",
        "print('Split sizes:', len(X_train), len(X_val), len(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Pipeline + hyperparameter search\n",
        "We keep the search small for speed. Tune **C** and **n-grams**; use class balancing for robustness on small sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "pipe = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('lr', LogisticRegression(max_iter=2000, class_weight='balanced', solver='liblinear'))\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'tfidf__ngram_range': [(1,1),(1,2)],\n",
        "    'tfidf__min_df': [1, 2],\n",
        "    'lr__C': [0.5, 1.0, 2.0]\n",
        "}\n",
        "\n",
        "search = GridSearchCV(pipe, param_grid, scoring='f1', cv=5, n_jobs=-1, verbose=0)\n",
        "search.fit(X_train, y_train)\n",
        "print('Best params:', search.best_params_)\n",
        "best = search.best_estimator_\n",
        "\n",
        "# Evaluate on validation to simulate model selection feedback\n",
        "val_pred = best.predict(X_val)\n",
        "val_proba = best.predict_proba(X_val)[:,1] if hasattr(best, 'predict_proba') else None\n",
        "val_report = classification_report(y_val, val_pred, digits=3)\n",
        "print('Validation report:\\n', val_report)\n",
        "val_auc = roc_auc_score(y_val, val_proba) if val_proba is not None and len(np.unique(y_val))==2 else float('nan')\n",
        "print('Validation ROC-AUC:', round(val_auc, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Final test evaluation\n",
        "We report accuracy, precision/recall/F1, ROC-AUC, and confusion matrix. Plots are optional for quick inspection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "test_pred = best.predict(X_test)\n",
        "test_proba = best.predict_proba(X_test)[:,1] if hasattr(best, 'predict_proba') else None\n",
        "print('Test report:\\n', classification_report(y_test, test_pred, digits=3))\n",
        "cm = confusion_matrix(y_test, test_pred)\n",
        "print('Confusion matrix:\\n', cm)\n",
        "\n",
        "if test_proba is not None and len(np.unique(y_test))==2:\n",
        "    auc = roc_auc_score(y_test, test_proba)\n",
        "    fpr, tpr, _ = roc_curve(y_test, test_proba)\n",
        "    print('ROC-AUC:', round(auc, 3))\n",
        "    # Optional quick plot\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, label=f'AUC={auc:.3f}')\n",
        "    plt.plot([0,1],[0,1],'--')\n",
        "    plt.xlabel('FPR'); plt.ylabel('TPR'); plt.title('ROC Curve')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Save model + summary\n",
        "Artifacts go to `outputs/` so you can track different runs by `RUN_ID`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "model_path = OUT_DIR / f'sentiment_model_{RUN_ID}.joblib'\n",
        "joblib.dump(best, model_path)\n",
        "print('Saved model →', model_path)\n",
        "\n",
        "summary = {\n",
        "    'run_id': RUN_ID,\n",
        "    'timestamp_utc': datetime.utcnow().isoformat() + 'Z',\n",
        "    'train_size': len(X_train), 'val_size': len(X_val), 'test_size': len(X_test),\n",
        "    'best_params': search.best_params_,\n",
        "    'val_f1_macro': float(np.mean(list(json.loads(json.dumps(classification_report(y_val, val_pred, output_dict=True)))['macro avg'].values())) if len(np.unique(y_val))>1 else 0.0),\n",
        "    'test_report': classification_report(y_test, test_pred, output_dict=True),\n",
        "}\n",
        "if test_proba is not None and len(np.unique(y_test))==2:\n",
        "    summary['test_auc'] = float(roc_auc_score(y_test, test_proba))\n",
        "\n",
        "summary_path = OUT_DIR / f'sentiment_model_summary_{RUN_ID}.json'\n",
        "with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print('Saved summary →', summary_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Inference helper + error analysis\n",
        "A small `infer()` wrapper and a peek at misclassified examples ranked by confidence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def infer(texts):\n",
        "    model = joblib.load(model_path)\n",
        "    probs = model.predict_proba(texts)[:,1]\n",
        "    preds = (probs >= 0.5).astype(int)\n",
        "    return list(zip(texts, preds.tolist(), probs.tolist()))\n",
        "\n",
        "# Demo\n",
        "print(infer(['excellent support and fast response', 'buggy app with terrible UX']))\n",
        "\n",
        "# Error analysis on test set\n",
        "if test_proba is not None:\n",
        "    margins = np.abs(test_proba - 0.5)\n",
        "    mis_idx = np.where(test_pred != y_test)[0]\n",
        "    hardest = sorted(mis_idx, key=lambda i: margins[i])[:10]\n",
        "    examples = [{'text': X_test[i], 'true': int(y_test[i]), 'pred': int(test_pred[i]), 'prob': float(test_proba[i])} for i in hardest]\n",
        "    pd.DataFrame(examples)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}