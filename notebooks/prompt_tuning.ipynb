{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üß† Prompt Engineering 101\n",
        "\n",
        "This notebook explores foundational patterns in **prompt design**, including:\n",
        "- System vs user role separation\n",
        "- Few-shot examples for controlled behavior\n",
        "- Temperature and sampling parameters\n",
        "- Simple evaluation harness for consistency\n",
        "\n",
        "It is designed as a teaching and experimentation scaffold for AI engineers studying prompt behavior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Imports and configuration\n",
        "import json, random, uuid, os\n",
        "from datetime import datetime\n",
        "\n",
        "OUTPUT_DIR = os.path.join('outputs')\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "def build_prompt(system, user, shots=None):\n",
        "    shots = shots or []\n",
        "    return {\n",
        "        'id': str(uuid.uuid4()),\n",
        "        'system': system.strip(),\n",
        "        'shots': shots,\n",
        "        'user': user.strip(),\n",
        "        'created': datetime.utcnow().isoformat() + 'Z'\n",
        "    }\n",
        "\n",
        "example = build_prompt(\n",
        "    system='You are a careful and precise software QA engineer.',\n",
        "    user='List 3 strategies to test a login page.',\n",
        "    shots=[{'q': 'How do you test form validation?', 'a': 'Use both valid and invalid inputs to confirm correct behavior.'}]\n",
        ")\n",
        "print(json.dumps(example, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1Ô∏è‚É£ Few-Shot Prompting\n",
        "\n",
        "Few-shot prompting provides models with examples to shape response tone, format, and reasoning structure.\n",
        "We can simulate prompt generation and compare consistency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "few_shot_examples = [\n",
        "    {'q': 'What is regression testing?', 'a': 'Re-running test cases after code changes to ensure existing functionality still works.'},\n",
        "    {'q': 'Explain smoke testing.', 'a': 'Basic checks to verify key functions work before deeper testing.'}\n",
        "]\n",
        "\n",
        "prompt_obj = build_prompt(\n",
        "    system='You are an experienced QA engineer providing concise, structured answers.',\n",
        "    user='Define exploratory testing.',\n",
        "    shots=few_shot_examples\n",
        ")\n",
        "\n",
        "print(json.dumps(prompt_obj, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2Ô∏è‚É£ Prompt Variants (Templates)\n",
        "\n",
        "Different templates can emphasize tone, role, or domain knowledge. Below are examples of structured variations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "templates = [\n",
        "    'You are an expert {role}. Provide clear, concise answers.',\n",
        "    'Act as a {role} mentor guiding a new team member.',\n",
        "    'As a senior {role}, respond in checklist format with reasoning.',\n",
        "    'Write your response as if you were documenting best practices for {role}s.'\n",
        "]\n",
        "\n",
        "roles = ['Data Scientist', 'QA Engineer', 'AI Ethics Specialist', 'Prompt Engineer']\n",
        "generated_templates = [t.format(role=r) for r in roles for t in templates]\n",
        "\n",
        "for t in generated_templates[:4]:\n",
        "    print('-', t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3Ô∏è‚É£ Evaluation Harness (Mock)\n",
        "\n",
        "Since we can‚Äôt call APIs directly here, this section simulates how you might **score prompt outputs** using clarity, completeness, and tone metrics. In a real workflow, you‚Äôd parse model responses and evaluate automatically."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def evaluate_prompt(prompt_text):\n",
        "    metrics = {\n",
        "        'clarity': random.uniform(0.7, 1.0),\n",
        "        'structure': random.uniform(0.6, 1.0),\n",
        "        'tone_consistency': random.uniform(0.5, 1.0)\n",
        "    }\n",
        "    score = sum(metrics.values()) / len(metrics)\n",
        "    return round(score, 3), metrics\n",
        "\n",
        "scores = []\n",
        "for idx, tmpl in enumerate(generated_templates[:8]):\n",
        "    s, m = evaluate_prompt(tmpl)\n",
        "    scores.append({'template': tmpl, 'score': s, 'metrics': m})\n",
        "\n",
        "print(json.dumps(scores[:3], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4Ô∏è‚É£ Parameter Sweeps\n",
        "\n",
        "We can log how model temperature and max token values affect consistency. (Simulated here for demo.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "temperatures = [0.2, 0.5, 0.7, 1.0]\n",
        "max_tokens = [100, 250, 500]\n",
        "\n",
        "results = []\n",
        "for t in temperatures:\n",
        "    for m in max_tokens:\n",
        "        avg_score = round(random.uniform(0.6, 1.0), 3)\n",
        "        results.append({'temperature': t, 'max_tokens': m, 'avg_eval_score': avg_score})\n",
        "\n",
        "print(json.dumps(results[:5], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5Ô∏è‚É£ Export Results & Summary\n",
        "\n",
        "We log all results with timestamps for further analysis or dashboard integration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "summary = {\n",
        "    'run_id': str(uuid.uuid4()),\n",
        "    'timestamp': datetime.utcnow().isoformat() + 'Z',\n",
        "    'total_templates': len(generated_templates),\n",
        "    'evaluated': len(scores),\n",
        "    'avg_score': round(sum(s['score'] for s in scores)/len(scores), 3)\n",
        "}\n",
        "\n",
        "summary_path = os.path.join(OUTPUT_DIR, 'prompt_tuning_summary.json')\n",
        "with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "    json.dump({'summary': summary, 'results': scores, 'params': results}, f, indent=2)\n",
        "\n",
        "print('Saved summary to ‚Üí', summary_path)\n",
        "json.dumps(summary, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß© Next Steps\n",
        "- Integrate this with an actual LLM API (e.g., OpenAI, Anthropic, or local model)\n",
        "- Add human-in-the-loop evaluation for qualitative feedback\n",
        "- Introduce versioning for prompt templates and output samples\n",
        "- Correlate prompt structure with model temperature for optimization"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}